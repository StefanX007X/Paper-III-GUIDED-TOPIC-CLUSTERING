{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54045dff-240b-496d-abe3-32a57bff9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# Import functions from other ipynb files\n",
    "import importlib\n",
    "import ipynb\n",
    "from ipynb.fs.full.utilities import CleanNews, CleanNews_w2v, ToPercent, FilterNews, Dataloader, GetTotalExamples\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm().pandas()\n",
    "\n",
    "# Downlaod the NLP Model: https://spacy.io/models/en\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#path = \"C:/Users/Stefa/Documents/Sentiment Analysis Files/FX/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b5eb8e-4c82-4af1-8f76-59d62ea95cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload Module\n",
    "importlib.reload(ipynb.fs.full.utilities)\n",
    "from ipynb.fs.full.utilities import CleanNews, CleanNews_w2v, ToPercent, FilterNews, Dataloader, GetTotalExamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "946a3658-8a71-4532-b706-609c38d78b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextualFactors_preprocessing():\n",
    "    \"\"\"\n",
    "    This class contains all necessary preprocessing tasks for the Textual Factors class.\n",
    "    Methods:\n",
    "    clean_raw_news:        Clean raw news articles for subesquent tasks\n",
    "    gen_w2v_training_data: Cleans raw news articles for the use of training the word2vec model. Output is a text file \n",
    "                           containing one sentence per line. Optionally it allows to generate also bigrams and trigrams\n",
    "                           with the gensim Phraser model.\n",
    "    generate_grams:        Generate bigrams and trigrams from unigrams\n",
    "    train_word2vec:        Train the word2vec model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, raw_data_dir, docs_dir, w2v_data_src):\n",
    "        self.root_dir     = root_dir            # Textual Factors root directory\n",
    "        self.raw_data_dir = raw_data_dir        # directory that contains the raw news articles\n",
    "        self.docs_dir     = docs_dir            # directory that contains the cleaned news articles\n",
    "        self.w2v_data_src = w2v_data_src        # directory that contains the word2vec training files\n",
    "        \n",
    "        Path(self.root_dir+'/models/').mkdir(parents=True, exist_ok=True) \n",
    "        Path(self.docs_dir).mkdir(parents=True, exist_ok=True) \n",
    "        Path(self.w2v_data_src).mkdir(parents=True, exist_ok=True) \n",
    "    \n",
    "    \n",
    "    def clean_raw_news(self, start_year=2011, end_year=2016, filename='news_data_fx_v2_####'):\n",
    "        print(f\"Clean raw news articles ... {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        \n",
    "        for year in list(np.arange(start_year, end_year)):\n",
    "            year = str(year)\n",
    "            print(year)\n",
    "            news = pd.read_csv(self.raw_data_dir + re.sub(r'####', str(year), filename)+'.csv', index_col=0, encoding='utf-8')  \n",
    "            #raw_news = raw_news.reset_index(drop=True)\n",
    "\n",
    "            # Drop all news with duplicate headlines and keep the most recent one\n",
    "            news = news[news.duplicated(subset='Headline', keep='first') == False]\n",
    "            news = news.reset_index(drop=True)\n",
    "\n",
    "            # Clean Headlines and Body\n",
    "            news.loc[:, 'Headline'] = news['Headline'].progress_apply(CleanNews, to_lowercase=True)\n",
    "            news.loc[:, 'Body']     =     news['Body'].progress_apply(CleanNews, to_lowercase=True)\n",
    "\n",
    "            # Convert price changes with the pattern 'to XX$ from XY$' to 'by YX percent'    \n",
    "            news.loc[:, 'Headline'] = news['Headline'].progress_apply(ToPercent)\n",
    "            news.loc[:, 'Body']     =     news['Body'].progress_apply(ToPercent)\n",
    "\n",
    "            # Save to csv\n",
    "            news.to_csv(self.docs_dir + re.sub(r'####', str(year), filename)+'_clean.csv', encoding='utf-8', index=True)\n",
    "               \n",
    "    \n",
    "    class tiny_dataloader(object):\n",
    "        def __init__(self, file_dir, fname):\n",
    "            self.file_dir = file_dir\n",
    "            self.fname    = fname\n",
    "            \n",
    "        def __iter__(self):    \n",
    "            for i, line in enumerate(open(os.path.join(self.file_dir, self.fname))):\n",
    "                yield line.split()\n",
    "    \n",
    "    \n",
    "    def generate_grams(self, src_dir, output_dir, model_dir, min_count, threshold, gram_type='bigram'):\n",
    "        \"\"\"\n",
    "        Generate bigrams or trigrams with gensim Phrases.\n",
    "        src_dir:    directory that contains files with unigrams (to generate bigrams) or files with bigrams (to generate trigrams)\n",
    "        output_dir: directory to save the generated bigram and trigram files\n",
    "        model_dir:  directory to save the Phrase model\n",
    "        min_count:  (float, optional) – Ignore all words and bigrams with total collected count lower than this value.\n",
    "        threshold:  (float, optional) – Represent a score threshold for forming the phrases (higher means fewer phrases). \n",
    "                    A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold.\n",
    "        gram_type:  select either 'bigram' or 'trigram', default='bigram'\n",
    "        \"\"\"  \n",
    "        files = os.listdir(src_dir)\n",
    "        years = []\n",
    "        for i, fname in enumerate(files):\n",
    "            year = re.findall(r\"\\d\\d\\d\\d\", fname)[0]\n",
    "            years.append(year)\n",
    "            \n",
    "        print(f'Train the {gram_type} Phraser model ...')\n",
    "        documents = Dataloader(path=src_dir, split=True)\n",
    "        model     = Phrases(documents, min_count=min_count, threshold=threshold, delimiter='_')\n",
    "\n",
    "        # Export the trained model = use less RAM, faster processing. Model updates no longer possible.\n",
    "        frozen_model = model.freeze()     \n",
    "        frozen_model.save(model_dir+gram_type+\"_phrase_model_\"+years[0]+'-'+years[-1]+\".pkl\")          \n",
    "        #phraser   = Phraser(model)      \n",
    "            \n",
    "        print(f'Generate {gram_type} training files ...')\n",
    "        for i, fname in enumerate(files):\n",
    "            year = re.findall(r\"\\d\\d\\d\\d\", fname)[0]\n",
    "            documents = self.tiny_dataloader(file_dir=src_dir, fname=fname)\n",
    "            \n",
    "            with open(output_dir + \"w2v_\"+gram_type+\"_phrases_\"+year+\".txt\", 'w') as f:\n",
    "                for i, sent in enumerate(documents):\n",
    "                    f.write(' '.join(frozen_model[sent]))\n",
    "                    f.write('\\n')\n",
    "\n",
    "            \n",
    "        \n",
    "    def gen_w2v_training_data(self, start_year=2011, end_year=2016, filename='news_data_fx_v2_####', \n",
    "                               min_sent_len=5, generate_unigrams=True, generate_bigrams=True, generate_trigrams=True,\n",
    "                               bigram_min_count=50, bigram_threshold=20, trigram_min_count=50, \n",
    "                               trigram_threshold=200):\n",
    "        \n",
    "        \"\"\"\n",
    "        Clean News for word2vec and write to .txt files\n",
    "        min_sent_len: Minium number of words that are considered as a sentence and are wirtten to the .txt files\n",
    "        \"\"\"\n",
    "        \n",
    "        Path(self.w2v_data_src+'/unigrams').mkdir(parents=True, exist_ok=True) \n",
    "                    \n",
    "        if generate_unigrams:\n",
    "            print(f\"Clean raw news articles for word2vec ... {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            for year in list(np.arange(start_year, end_year)):\n",
    "                year = str(year)\n",
    "                print(year)\n",
    "\n",
    "                raw_news = pd.read_csv(self.raw_data_dir + re.sub(r'####', str(year), filename)+'.csv', index_col=0, encoding='utf-8')  \n",
    "\n",
    "                # Drop all news that contain no body text\n",
    "                raw_news = raw_news[raw_news.Body.isna() == False]\n",
    "\n",
    "                # Drop all news with duplicate headlines and keep the most recent one\n",
    "                raw_news = raw_news[raw_news.duplicated(subset='Headline', keep='last') == False]\n",
    "                raw_news = raw_news.reset_index(drop=True)\n",
    "\n",
    "                w2v_train      = pd.DataFrame(columns=['Body'])\n",
    "                w2v_train.Body = raw_news.Body.progress_apply(CleanNews_w2v, to_lowercase=True)\n",
    "                w2v_train      = w2v_train.dropna().reset_index(drop=True)  \n",
    "\n",
    "                with open(self.w2v_data_src+'/unigrams/w2v_train_'+year+'.txt', 'w') as f:\n",
    "                    for article in w2v_train.Body:\n",
    "                        l = re.split('(?<=\\w\\w)\\.\\s|\\s\\.\\s|\\.$|^\\.|\\*', article)\n",
    "                        for s in l:\n",
    "                            if len(s.split(' ')) > min_sent_len:\n",
    "                                f.write('%s\\n' % s.strip())\n",
    "        \n",
    "        \n",
    "        if generate_bigrams:\n",
    "            print(f\"Generate bigrams ... {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            Path(self.w2v_data_src+'/bigrams').mkdir(parents=True, exist_ok=True) \n",
    "            self.generate_grams(src_dir    = self.w2v_data_src+'unigrams/', \n",
    "                                output_dir = self.w2v_data_src+'bigrams/',\n",
    "                                model_dir  = self.root_dir+'models/',\n",
    "                                min_count  = bigram_min_count, \n",
    "                                threshold  = bigram_threshold,\n",
    "                                gram_type  ='bigram'\n",
    "                               )\n",
    "                          \n",
    "        if generate_trigrams:\n",
    "            print(f\"Generate trigrams ... {datetime.now().strftime('%H:%M:%S')}\")\n",
    "            Path(self.w2v_data_src+'/trigrams').mkdir(parents=True, exist_ok=True) \n",
    "            self.generate_grams(src_dir    = self.w2v_data_src+'bigrams/', \n",
    "                                output_dir = self.w2v_data_src+'trigrams/',\n",
    "                                model_dir  = self.root_dir+'models/',\n",
    "                                min_count  = trigram_min_count, \n",
    "                                threshold  = trigram_threshold,\n",
    "                                gram_type  ='trigram'\n",
    "                               )\n",
    "                               \n",
    "                \n",
    "    def train_word2vec(self, w2v, model_name, gram_type='trigrams', epochs=10):\n",
    "        \"\"\"\n",
    "        Train a Word2Vec model\n",
    "        w2v:       dictionary containing the word2vec training parameters\n",
    "        gram_type: select either 'unigrams', 'bigrams', 'trigrams'\n",
    "        epochs:    number of epochs to train word2vec\n",
    "        \"\"\"\n",
    "        print(f\"Train the word2vec model ... {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        examples  = GetTotalExamples(self.w2v_data_src+gram_type)\n",
    "        sentences = Dataloader(self.w2v_data_src+gram_type, print_epoch=True, split=True) \n",
    "        model     = Word2Vec(sg=w2v['sg'], hs=w2v['hs'], vector_size=w2v['size'], \n",
    "                             negative=w2v['negative'],   window=w2v['window'], \n",
    "                             min_count=w2v['min_count'], alpha=w2v['alpha'],\n",
    "                             min_alpha=w2v['min_alpha'], workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "        # Train the model\n",
    "        print('Build vocabulary ...')\n",
    "        model.build_vocab(sentences)\n",
    "\n",
    "        print('Train ...')\n",
    "        model.train(sentences, total_examples=examples, epochs=epochs)  # optimal: 80 epochs\n",
    "\n",
    "        # Save the model\n",
    "        print('Save the model ...')\n",
    "        model.save(self.root_dir+'/models/'+model_name+'.word2vec')        \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d02b9d6-54d6-40d1-ab33-de66ac4f157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pre = TextualFactors_preprocessing(\n",
    "    root_dir     = \"C:/Users/Stefa/Documents/Uni/Projektassistenz/FX/Python/Textual_Factors/\",\n",
    "    raw_data_dir = \"F:/Sentiment Analysis Files/FX/\",\n",
    "    docs_dir     = \"F:/Sentiment Analysis Files/FX/clean_fx_news_v2/\",\n",
    "    w2v_data_src = \"F:/Sentiment Analysis Files/FX/train_w2v_data_2016-2019/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44fb10a-1c0d-4b45-b5e1-712e71b8e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pre.clean_raw_news(\n",
    "    start_year = 2006, \n",
    "    end_year   = 2011, \n",
    "    filename   = 'news_data_fx_v2_####'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84a86827-6369-44e7-a9a6-ccdeb4bed1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate bigrams ... 16:32:47\n",
      "Train the bigram Phraser model ...\n",
      "Generate bigram training files ...\n",
      "Generate trigrams ... 16:51:03\n",
      "Train the trigram Phraser model ...\n",
      "Generate trigram training files ...\n"
     ]
    }
   ],
   "source": [
    "tf_pre.gen_w2v_training_data(\n",
    "    start_year = 2016, \n",
    "    end_year   = 2019, \n",
    "    filename   = 'news_data_fx_v2_####', \n",
    "    min_sent_len = 5, \n",
    "    generate_unigrams = False, \n",
    "    generate_bigrams  = True, \n",
    "    generate_trigrams = False,\n",
    "    bigram_min_count  = 50, \n",
    "    bigram_threshold  = 10, \n",
    "    trigram_min_count = 50, \n",
    "    trigram_threshold = 150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa230b52-e0c1-47fc-a231-c2a0948cd933",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_pre.train_word2vec(\n",
    "    w2v = {'sg':0, 'hs':0, 'size':32, 'negative':5, 'window':8, 'min_count':50, 'alpha':0.03, 'min_alpha':0.005},\n",
    "    model_name = 'w2v_cbow_32_neg_5_window_8_40_epochs_trigrams_2016_2019',\n",
    "    gram_type  = 'trigrams',\n",
    "    epochs     = 40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bee9c2-c1bc-4ab0-bb6c-4299d158ec79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "365fa0f8-8ed7-48af-9be7-d45b65c30ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 CO2 in Columns: True\n",
      "2007 CO2 in Columns: True\n",
      "2008 CO2 in Columns: True\n",
      "2009 CO2 in Columns: True\n",
      "2010 CO2 in Columns: True\n",
      "2011 CO2 in Columns: False\n",
      "2012 CO2 in Columns: False\n",
      "2013 CO2 in Columns: False\n",
      "2014 CO2 in Columns: False\n",
      "2015 CO2 in Columns: False\n",
      "2016 CO2 in Columns: True\n",
      "2017 CO2 in Columns: True\n",
      "2018 CO2 in Columns: True\n",
      "2019 CO2 in Columns: True\n"
     ]
    }
   ],
   "source": [
    "raw_data_dir = \"F:/Sentiment Analysis Files/FX/\"\n",
    "filename   = 'news_data_fx_v2_####'\n",
    "\n",
    "for year in range(2006, 2020, 1):\n",
    "    news = pd.read_csv(raw_data_dir + re.sub(r'####', str(year), filename)+'.csv', index_col=0, encoding='utf-8', nrows=5)  \n",
    "    print(year, f\"CO2 in Columns: {'CO2' in set(news.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8620bd6-cd51-48bf-a43a-c9d4bc9664fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tiny_dataloader(object):\n",
    "    def __init__(self, file_dir, fname):\n",
    "        self.file_dir = file_dir\n",
    "        self.fname    = fname\n",
    "\n",
    "    def __iter__(self):    \n",
    "        for i, line in enumerate(open(os.path.join(self.file_dir, self.fname))):\n",
    "            yield line\n",
    "            \n",
    "f = tiny_dataloader(\"F:/Sentiment Analysis Files/FX/train_w2v_data_1996-2018/bigrams/\", \"w2v_bigram_phrases_2002.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40471b46-bc85-4b3b-830f-7888b8d7a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, text in enumerate(f):\n",
    "    print(text)\n",
    "    \n",
    "    if i >= 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dcfc75-97ce-483f-ba03-e21858d8cdd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
